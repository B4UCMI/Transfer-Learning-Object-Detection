# -*- coding: utf-8 -*-
"""Pre-Final Copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13J1FL0amC3zFbQymRw2E8k74XItx9ujg

IMPORTING LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
import tqdm
!pip install tensorboard
!pip install tensorboard
!pip install tensorboardX
!pip install torchmetrics
# %load_ext tensorboard
from tensorboardX import SummaryWriter
from torchmetrics import Specificity
from torch.utils.tensorboard import SummaryWriter
from numpy import vstack
from pandas import read_csv
import torch, torchvision
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import time
from torchsummary import summary
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
from __future__ import print_function, division
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import matplotlib.pyplot as plt
import time
import copy
from sklearn.metrics import f1_score  
cudnn.benchmark = True
plt.ion()   # interactive mode
import numpy as np
import matplotlib.pyplot as plt
!pip install timm
import timm

"""MOUNTING TO GOOGLE DRIVE"""

from google.colab import drive
drive.mount('/content/drive')

"""DATA DIRECTORY"""

data_dir = '/content/drive/MyDrive/T_L - Copy - Copy - Copy'

"""SPECIFY NO. OF CLASSES





"""

num_class=4

"""No. Of EPOCHS"""

numepo=10

"""DISPLAYING THE IMAGES"""

def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated

"""FUNCTION TO TRAIN THE MODEL"""

def train_model(model, criterion, optimizer, scheduler, num_epochs=numepo):
    since = time.time()
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0
    p=0
    writer = SummaryWriter('/content/drive/MyDrive/Graphs')
    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)
        p=p+1
        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            l=0;
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    model=model.to(device)
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]             
            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))
            # deep copy the model
            if phase == 'train':
              writer.add_scalar('Train/Loss', epoch_loss, p) 
              writer.add_scalar('Train/Acc', epoch_acc,p ) 
            if phase == 'val':
              writer.add_scalar('Validation/Loss', epoch_loss, p) 
              writer.add_scalar('Validation/Acc', epoch_acc,p ) 
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())  
            l=l+1;
        print()
    writer.flush()
    writer.close()
    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))
    # load best model weights
    model.load_state_dict(best_model_wts)
    return model

"""FUNCTION TO TEST THE MODEL"""

def test_model(model, criterion, optimizer, scheduler):
    for phase in ['test']:
      model.eval()
      running_loss = 0.0
      running_corrects = 0
      f1_sc=[]
      for inputs, labels in dataloaders[phase]:
        inputs = inputs.to(device)
        labels = labels.to(device)
        with torch.no_grad():
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)       
        running_loss += loss.item() * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)  
        f1_sc.append(f1_score(labels.cpu().data, preds.cpu(),average='weighted')) 
        epoch_loss = running_loss / dataset_sizes[phase]
      f1_sc = np.mean(f1_sc)  
      epoch_acc = running_corrects.double() / dataset_sizes[phase]                           
      print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) 
      print('F1 Score',f1_sc)

"""RESNET 152"""

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])    
}
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val', 'test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=3,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val', 'test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}
class_names = image_datasets['train'].classes
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)
imshow(out, title=[class_names[x] for x in classes])

modelA = models.resnet152(pretrained=True)
for param in modelA.parameters():
    param.requires_grad_(True)
modelA= modelA.to(device)    
num_ftrs = modelA.fc.in_features
modelA.fc=nn.Linear(num_ftrs,num_class)
criterion = nn.CrossEntropyLoss()
optimizerA = optim.Adam(modelA.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)
exp_lr_scheduler = lr_scheduler.StepLR(optimizerA, step_size=4, gamma=0.2)    
modelA = train_model(modelA, criterion, optimizerA, exp_lr_scheduler, num_epochs=numepo)
      
test_model(modelA, criterion, optimizerA, exp_lr_scheduler)

"""GRAPH PLOTTING"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/drive/MyDrive/Graphs'

"""XCEPTION"""

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])    
}
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val', 'test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=3,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val', 'test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}
class_names = image_datasets['train'].classes
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)
imshow(out, title=[class_names[x] for x in classes])

modelB=timm.create_model('xception', pretrained=True, num_classes=num_class)
for param in modelB.parameters():
    param.requires_grad_(True)
modelB= modelB.to(device)    
criterion = nn.CrossEntropyLoss()
optimizerB = optim.Adam(modelB.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)
exp_lr_scheduler = lr_scheduler.StepLR(optimizerB, step_size=4, gamma=0.2)    
modelB = train_model(modelB, criterion, optimizerB, exp_lr_scheduler, num_epochs=numepo)
      
test_model(modelB, criterion, optimizerB, exp_lr_scheduler)

"""GRAPH PLOTTING"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/drive/MyDrive/Graphs'

"""EFFICIENTNET_B5"""

data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(456),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(480),
        transforms.CenterCrop(456),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(size=456),
        transforms.CenterCrop(size=456),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])    
}
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val', 'test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=3,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val', 'test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}
class_names = image_datasets['train'].classes
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)
imshow(out, title=[class_names[x] for x in classes])

modelC=torchvision.models.efficientnet_b5(pretrained=True)
for param in modelC.parameters():
    param.requires_grad_(True)
modelC= modelC.to(device)    
num_ftrs = modelC.classifier[1].in_features
modelC.classifier[1]=nn.Linear(num_ftrs,num_class)
criterion = nn.CrossEntropyLoss()
optimizerC = optim.Adam(modelC.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)
exp_lr_scheduler = lr_scheduler.StepLR(optimizerC, step_size=4, gamma=0.2)    
modelC = train_model(modelC, criterion, optimizerC, exp_lr_scheduler, num_epochs=numepo)
      
test_model(modelC, criterion, optimizerC, exp_lr_scheduler)

"""GRAPH PLOTTING"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/drive/MyDrive/Graphs'

"""ENSEMLE_LEARNING"""

# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(456),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(480),
        transforms.CenterCrop(456),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'test': transforms.Compose([
        transforms.Resize(size=456),
        transforms.CenterCrop(size=456),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406],
                             [0.229, 0.224, 0.225])
    ])    
}
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val', 'test']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=3,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val', 'test']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}
class_names = image_datasets['train'].classes
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))
# Make a grid from batch
out = torchvision.utils.make_grid(inputs)
imshow(out, title=[class_names[x] for x in classes])

import torch.nn.functional as F
class MyEnsemble(nn.Module):
    def __init__(self, modelA, modelB,modelC, nb_classes=10):
        super(MyEnsemble, self).__init__()
        self.modelA = modelA
        self.modelB = modelB
        self.modelC = modelC
        # Remove last linear layer
        self.modelA.fc = nn.Identity()
        self.modelB.fc = nn.Identity()
        self.modelC.fc = nn.Identity()
        
        # Create new classifier
        self.classifier = nn.Linear(4100, nb_classes)    #The numbere (4100) is to be specified each time
        
    def forward(self, x):
        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods
        x1 = x1.view(x1.size(0), -1)
        x2 = self.modelB(x)
        x2 = x2.view(x2.size(0), -1)
        x3 = self.modelC(x)
        x3 = x3.view(x3.size(0), -1)
        x = torch.cat((x1, x2,x3), dim=1)
        x = self.classifier(F.relu(x))
        return x

# Train your separate models
for param in modelA.parameters():
    param.requires_grad_(False)
modelA= modelA.to(device)    

for param in modelB.parameters():
    param.requires_grad_(False)
modelB = modelB.to(device)

for param in modelC.parameters():
    param.requires_grad_(False)
modelC= modelC.to(device)    
# Create ensemble model
model_conv = MyEnsemble(modelA, modelB,modelC,3*num_class)
model_conv = model_conv.to(device)
criterion = nn.CrossEntropyLoss()
optimizer_conv = optim.Adam(model_conv.classifier.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=4, gamma=0.2)    
model_nxt = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=numepo)
      
test_model(model_nxt, criterion, optimizer_conv, exp_lr_scheduler)

"""GRAPH PLOTTING"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/drive/MyDrive/Graphs'